# CarlaRL Policy-as-a-Service

High-performance serving infrastructure for CarlaRL reinforcement learning policies. This service transforms trained RL models into production-ready microservices with millisecond-latency inference capabilities.

## ğŸš€ Features

- **High-Performance Inference**: Sub-10ms P50 latency for single predictions on CPU
- **Batch Processing**: Efficient batch inference supporting 1000+ requests/sec
- **Production Ready**: Enterprise-grade serving with health checks, metrics, and observability
- **Model Management**: Standardized artifact format with semantic versioning
- **Memory Optimization**: Tensor pre-allocation and memory pinning for reduced latency
- **Caching**: Intelligent result caching for identical inputs
- **Deterministic Mode**: Reproducible inference for research and testing
- **Multi-format Support**: PyTorch and TorchScript model formats
- **Container Native**: Docker-first deployment with Kubernetes support

## ğŸ“‹ Quick Start

### Prerequisites

- Python 3.11+
- Docker (optional)
- CUDA-capable GPU (optional, for GPU inference)

### Local Development

1. **Clone and Setup**
   ```bash
   git clone <repository-url>
   cd carla-rl-serving
   make install-dev
   ```

2. **Create Example Artifacts**
   ```bash
   make create-artifacts
   ```

3. **Run Development Server**
   ```bash
   make dev
   ```

4. **Test the API**
   ```bash
   # Health check
   curl http://localhost:8080/healthz
   
   # Model metadata
   curl http://localhost:8080/metadata
   
   # Prediction
   curl -X POST http://localhost:8080/predict \
     -H "Content-Type: application/json" \
     -d '{
       "observations": [{
         "speed": 25.5,
         "steering": 0.1,
         "sensors": [0.8, 0.2, 0.5, 0.9, 0.1]
       }],
       "deterministic": true
     }'
   ```

### Docker Deployment

1. **Build and Run**
   ```bash
   make docker-build
   make docker-run
   ```

2. **Or use Docker Compose**
   ```bash
   docker-compose up -d
   ```

3. **With Monitoring Stack**
   ```bash
   docker-compose --profile monitoring up -d
   ```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Layer    â”‚â”€â”€â”€â”€â”‚  Model Manager   â”‚â”€â”€â”€â”€â”‚ Inference Engine â”‚
â”‚  (FastAPI)     â”‚    â”‚   (Loading)      â”‚    â”‚  (Optimization)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Validation    â”‚    â”‚   Preprocessing  â”‚    â”‚    Caching      â”‚
â”‚   & Schemas     â”‚    â”‚    Pipeline      â”‚    â”‚   & Metrics     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
carla-rl-serving/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ server.py          # FastAPI application
â”‚   â”œâ”€â”€ inference.py       # Inference engine
â”‚   â”œâ”€â”€ model_loader.py    # Model loading utilities
â”‚   â”œâ”€â”€ preprocessing.py   # Feature preprocessing
â”‚   â”œâ”€â”€ io_schemas.py      # Pydantic schemas
â”‚   â”œâ”€â”€ exceptions.py      # Custom exceptions
â”‚   â””â”€â”€ version.py         # Version management
â”œâ”€â”€ tests/                 # Test suite
â”‚   â”œâ”€â”€ test_*.py         # Unit tests
â”‚   â””â”€â”€ test_*_qa.py      # QA validation tests
â”œâ”€â”€ artifacts/             # Model artifacts
â”‚   â””â”€â”€ v0.1.0/           # Version directory
â”‚       â”œâ”€â”€ model.pt      # Model file
â”‚       â”œâ”€â”€ preprocessor.pkl # Preprocessor
â”‚       â””â”€â”€ model_card.yaml  # Metadata
â”œâ”€â”€ monitoring/            # Monitoring configuration
â”œâ”€â”€ k8s/                  # Kubernetes manifests
â”œâ”€â”€ scripts/              # Utility scripts
â”œâ”€â”€ Dockerfile            # Container definition
â”œâ”€â”€ docker-compose.yml    # Multi-service deployment
â”œâ”€â”€ pyproject.toml        # Python project config
â”œâ”€â”€ Makefile              # Development commands
â””â”€â”€ README.md             # This file
```

## ğŸ”§ API Reference

### Health & Metadata

- `GET /healthz` - Service health check
- `GET /metadata` - Model metadata and configuration
- `GET /metrics` - Prometheus metrics (optional)

### Inference

- `POST /predict` - Batch inference endpoint
- `POST /warmup` - Model warmup for optimization

### Request/Response Format

**Prediction Request:**
```json
{
  "observations": [
    {
      "speed": 25.5,
      "steering": 0.1,
      "sensors": [0.8, 0.2, 0.5, 0.9, 0.1]
    }
  ],
  "deterministic": false
}
```

**Prediction Response:**
```json
{
  "actions": [
    {
      "throttle": 0.7,
      "brake": 0.0,
      "steer": 0.1
    }
  ],
  "version": "v0.1.0",
  "timingMs": 8.5,
  "deterministic": false
}
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ARTIFACT_DIR` | `artifacts` | Directory containing model artifacts |
| `MODEL_VERSION` | `v0.1.0` | Model version to load |
| `USE_GPU` | `0` | Enable GPU inference (1 for enabled) |
| `PORT` | `8080` | Server port |
| `WORKERS` | `1` | Number of uvicorn workers |
| `LOG_LEVEL` | `info` | Logging level |
| `CORS_ORIGINS` | `*` | Allowed CORS origins |

### Model Artifacts

Each model version requires:
- `model.pt` - PyTorch/TorchScript model file
- `preprocessor.pkl` - Feature preprocessing pipeline
- `model_card.yaml` - Model metadata and configuration

**Example model_card.yaml:**
```yaml
model_name: "carla-ppo"
version: "v0.1.0"
model_type: "pytorch"
input_shape: [5]
output_shape: [3]
framework_version: "2.1.0"
description: "CarlaRL PPO policy for autonomous driving"
performance_metrics:
  reward: 850.5
  success_rate: 0.95
artifact_hashes:
  model.pt: "sha256:abc123..."
  preprocessor.pkl: "sha256:def456..."
```

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run with coverage
make test-cov

# Run integration tests
make test-integration

# Run performance tests
make test-performance

# Code quality checks
make quality
```

## ğŸ“Š Monitoring

The service exposes metrics compatible with Prometheus:

- Request latency percentiles (P50, P95, P99)
- Throughput (requests/sec, observations/sec)
- Error rates and counts
- Model version and status
- Memory usage statistics

### Grafana Dashboard

Use the included Grafana dashboard for visualization:
```bash
docker-compose --profile monitoring up -d
# Access Grafana at http://localhost:3000 (admin/admin)
```

## ğŸš¢ Deployment

### Kubernetes

```bash
# Deploy to Kubernetes
make deploy-k8s

# Scale deployment
kubectl scale deployment carla-rl-serving --replicas=3
```

### Performance Tuning

1. **CPU Optimization**
   - Use multiple workers: `WORKERS=4`
   - Enable tensor pre-allocation
   - Use TorchScript models

2. **GPU Optimization**
   - Set `USE_GPU=1`
   - Use memory pinning
   - Batch requests for efficiency

3. **Memory Management**
   - Configure cache size appropriately
   - Monitor memory usage with `/metrics`
   - Use resource limits in containers

## ğŸ” Troubleshooting

### Common Issues

1. **Model Loading Errors**
   ```bash
   # Check artifact integrity
   make validate-artifacts
   
   # Verify model format
   python -c "import torch; print(torch.jit.load('artifacts/v0.1.0/model.pt'))"
   ```

2. **Performance Issues**
   ```bash
   # Run benchmark
   make benchmark
   
   # Check metrics
   curl http://localhost:8080/metrics
   ```

3. **Memory Issues**
   ```bash
   # Monitor memory usage
   docker stats carla-rl-serving
   
   # Check for memory leaks
   make test-performance
   ```

### Logs

```bash
# View container logs
docker logs carla-rl-serving

# Follow logs in real-time
docker logs -f carla-rl-serving
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `make test`
5. Submit a pull request

### Development Workflow

```bash
# Setup development environment
make install-dev

# Run pre-commit hooks
pre-commit run --all-files

# Run full test suite
make test-all

# Build and test Docker image
make docker-build
make docker-run
```

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com/) for the high-performance web framework
- [PyTorch](https://pytorch.org/) for the machine learning framework
- [CARLA](https://carla.org/) for the autonomous driving simulator
- [Pydantic](https://pydantic.dev/) for data validation and settings management

---

**Built with â¤ï¸ for the autonomous driving research community**
